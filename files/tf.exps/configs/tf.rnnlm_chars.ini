init_scale: 0.05
learning_rate: 1.0
max_grad_norm: 5
num_layers: 2
num_steps: 100
hidden_size: 1000
max_epoch: 1
max_max_epoch: 15
keep_prob: 0.5
lr_decay: 0.8
batch_size: 20
vocab_size: 90
